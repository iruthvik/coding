import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._
import java.text.SimpleDateFormat
import java.text.ParseException

object StreamingJob {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("StreamingJob")
      .getOrCreate()

    // Define your schema
    val ElineRefschema = ???

    import spark.implicits._

    // Function to get the latest date partition path in GCS
    def getLatestDatePartitionPath(basePath: String): String = {
      val gcs = "gs://"
      val trimmedBasePath = if (basePath.startsWith(gcs)) basePath.drop(gcs.length) else basePath

      val dateFormats = Seq("yyyyMMdd") // Add more date formats if needed

      val dateFormat = new SimpleDateFormat("yyyyMMdd")
      val latestDatePartition = spark
        .read.text(s"$gcs$trimmedBasePath")
        .as[String]
        .collect()
        .flatMap { fileName =>
          dateFormats.flatMap { dateFormatPattern =>
            try {
              val date = dateFormat.parse(fileName)
              Some(date -> fileName)
            } catch {
              case _: ParseException => None
            }
          }
        }
        .reduceOption((a, b) => if (a._1.after(b._1)) a else b)
        .map(_._2)

      latestDatePartition match {
        case Some(partition) => s"$gcs$trimmedBasePath/$partition" // Include the latest partition
        case None => throw new RuntimeException("No valid date partitions found.")
      }
    }

    // Define a function to update the reference data
    def updateReferenceData(referencePath: String): Unit = {
      // Read the updated reference data into a new DataFrame
      val newReferenceDF = spark.read
        .option("delimiter", "|")
        .schema(ElineRefschema)
        .csv(referencePath)

      // Perform any necessary transformations on newReferenceDF

      // Register the new reference DataFrame as a temporary view
      newReferenceDF.createOrReplaceTempView("reference_data")
    }

    // Get the latest date partition dynamically
    val baseReferencePath = "gs://internal/cpa/date/" // Specify the base path to date partitions with trailing "/"
    val latestDatePartition = getLatestDatePartitionPath(baseReferencePath)

    // Update the reference data with the latest date partition
    updateReferenceData(latestDatePartition)

    val streamingDF = spark.readStream
      .format("pulsar")
      .option("topic", "cpa")
      .load()

    // Rest of your Spark Structured Streaming code
    // ...

    spark.stop()
  }
}
