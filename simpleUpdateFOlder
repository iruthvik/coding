import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

object StreamingJob {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("StreamingJob")
      .getOrCreate()

    // Define your schema
    val ElineRefschema = ???

    import spark.implicits._

    // Function to get the latest date partition path in GCS
    def getLatestDatePartitionPath(basePath: String): String = {
      val gcs = "gs://"
      val trimmedBasePath = if (basePath.startsWith(gcs)) basePath.drop(gcs.length) else basePath

      // List the GCS folders (date partitions)
      val datePartitions = spark.read.text(s"$gcs$trimmedBasePath").as[String].collect()

      if (datePartitions.isEmpty) {
        throw new RuntimeException("No date partitions found in GCS path.")
      }

      // Sort the date partitions by folder name (which is the date) in descending order
      val latestDatePartition = datePartitions.sorted.reverse.head

      s"$gcs$trimmedBasePath$latestDatePartition"
    }

    // Define a function to update the reference data
    def updateReferenceData(referencePath: String): Unit = {
      // Read the updated reference data into a new DataFrame
      val newReferenceDF = spark.read
        .option("delimiter", "|")
        .schema(ElineRefschema)
        .csv(referencePath)

      // Perform any necessary transformations on newReferenceDF

      // Register the new reference DataFrame as a temporary view
      newReferenceDF.createOrReplaceTempView("reference_data")
    }

    // Get the latest date partition dynamically
    val baseReferencePath = "gs://internal/cpa/date=yyyyMMdd" // Specify the base path to date partitions
    val latestDatePartition = getLatestDatePartitionPath(baseReferencePath)

    // Update the reference data with the latest date partition
    updateReferenceData(latestDatePartition)

    val streamingDF = spark.readStream
      .format("pulsar")
      .option("topic", "cpa")
      .load()

    // Rest of your Spark Structured Streaming code
    // ...

    spark.stop()
  }
}
