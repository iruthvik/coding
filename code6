import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._
import org.apache.spark.sql.streaming.Trigger

object StreamingJob {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("StreamingJob")
      .getOrCreate()

    // Define your schema
    val ElineRefschema = ???

    import spark.implicits._

    // Declare an empty DataFrame for reference data
    var referenceDF = spark.emptyDataFrame

    // Function to update reference data from GCS
    def refreshReference(gcsBucket: String, gcsBasePath: String): Unit = {
      referenceDF.unpersist()

      // Use the gsutil command to list the directories (date partitions) in the GCS bucket
      val listDirectoriesCommand = s"gsutil ls -d gs://$gcsBucket/$gcsBasePath*"
      val directoriesOutput = sys.process.Process(listDirectoriesCommand).lineStream

      // Find the latest directory (date partition) based on the directory names
      val latestDirectory = directoriesOutput
        .map(directory => directory.stripPrefix("gs://"))
        .maxOption

      latestDirectory match {
        case Some(dir) =>
          // Construct the path to the latest directory
          val latestDirPath = s"gs://$gcsBucket/$dir"

          // Read reference data from the latest directory
          val newReferenceDF = spark.read
            .option("delimiter", "|")
            .schema(ElineRefschema)
            .csv(latestDirPath)

          // Perform any necessary transformations on newReferenceDF

          // Persist the updated reference DataFrame
          referenceDF = newReferenceDF.persist()

        case None =>
          println("No reference data found in GCS.")
      }
    }

    // Initial reference data load
    val gcsBucket = "internal"
    val gcsBasePath = "cpa/date=" // Adjust the base path as needed
    refreshReference(gcsBucket, gcsBasePath)

    // Define a streaming source (e.g., Pulsar)
    val streamingDF = spark.readStream
      .format("pulsar")
      .option("topic", "cpa")
      .load()

    // Define your streaming query logic
    val query = streamingDF
      .writeStream
      .trigger(Trigger.ProcessingTime("1 day")) // Adjust the interval as needed
      .foreachBatch { (_, _) =>
        // Refresh the reference data in each batch
        refreshReference(gcsBucket, gcsBasePath)

        // Perform your join and other streaming operations using referenceDF
        val resultDF = streamingDF.join(referenceDF, $"streamingColumn" === $"referenceColumn")

        // Output or further process resultDF as needed
        // For example, you can write it to a sink or perform additional transformations.
      }
      .start()

    // Wait for the streaming query to terminate
    query.awaitTermination()

    spark.stop()
  }
}
